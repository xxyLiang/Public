### 参数方法和非参数方法

- **参数方法：**在参数化方法中，我们通常根据先验知识假设模型服从函数f的分布，然后利用训练集估计出模型参数。
  - 参数化方法最大的缺点是，我们所做的**假设可能并不总是正确**的。例如，你可以假设函数的形式是线性的，但实际上它并不是。因此这些方法涉及较不灵活的算法，通常用于解决一些不复杂的问题。
  - 参数化方法**速度非常快**，而且它们**需要的数据也少得多**。此外，由于参数化方法虽然不太灵活但是因为基于我们做出的假设，所以它们更容易解释。
  - 机器学习中的参数化方法包括**线性判别分析、朴素贝叶斯和感知器**。
  
- **非参数方法：**指的是对于要估计的函数的形式不做任何潜在的假设的一组算法。由于没有做任何假设，这种方法可以估计未知函数f的任何形式。
  - 非参数方法往往**更精确**，因为它们寻求最佳拟合数据点。但是这是以需要进行**大量的观测为代价**的（这些观测是精确估计未知函数f所必需的）。并且这些方法在训练模型时往往效率较低。
  - 另外的一个问题是，非参数方法有时可能会引入**过拟合**，因为由于这些算法更灵活，它们有时可能会以无法很好地泛化到新的、看不见的数据点的方式学习错误和噪声。
  - 非参数方法**非常灵活**，因为没有对底层函数做出任何假设，所以可以带来**更好的模型性能**。
  - 机器学习中一些非参数方法的例子包括支持**决策树、向量机和kNN**。



### 模型的过分拟合

**分类模型的误差大致分为两种：**

- **训练误差**：也称再代入误差或表现误差，是在训练记录上误分类样本比例。
- **泛化误差**：是模型在未知记录上的期望误差。泛化误差的估计方法：
  - 使用训练误差（一种很差的估计）
  - 训练误差结合模型复杂度，相同训练误差但复杂度更小的模型更好
  - 训练误差的统计修正来估计
  - 使用验证集：保持方法、随机二次抽样、交叉验证、自助法（bootstrap）

过分拟合是指训练误差还在继续降低，但检验误差开始增大。

过分拟合的导致因素：噪声、缺乏代表性样本



### 数据变换

归一化： $x'=\frac{x-min(x)}{max(x)-min(x)}$

标准化： $x'=\frac{x-\bar{x}}{\sigma}$

中心化： $x'=x-\bar{x}$ ，变换后特征x'的均值为0

log变换 ： $x'=\frac{log(x)}{log(max(x))}$

sigmoid变换： $x'=\frac{1}{1+e^{-x}}$

softmax变换： $x'=\frac{e^x}{{\sum_{i=1}^n}e^{x_i}}$



#### 归一化和标准化的区别

- 归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。
- 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，会改变数据分布（均值和标准差改变，但分布类型不变），和整体样本分布相关，每个样本点都能对标准化产生影响。
- 它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。



#### 归一化和标准化的原因、用途

1. 统计建模中，如回归模型，自变量 $X$ 的量纲不一致导致了回归系数无法直接解读或者错误解读；需要将 $X$ 都处理到统一量纲下，这样才可比；
2. 机器学习任务和统计学任务中有很多地方要用到“距离”的计算，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；
3. 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。



#### 归一化和标准化的选择

- 如果你对处理后的数据范围有严格要求，那肯定是归一化。
- 如果数据不为稳定，存在极端的最大最小值，不要用归一化。
- 标准化是ML中更通用的手段，如果你无从下手，可以直接使用标准化；
- 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化表现更好；在不涉及距离度量、协方差计算的时候，可以使用归一化方法。



#### 是否所有情况都需要归一化和标准化

- 当原始数据不同维度特征的尺度（量纲）不一致时，需要标准化步骤对数据进行标准化或归一化处理，反之则不需要进行数据标准化。
- 也不是所有的模型都需要做归一的，比如模型算法里面有没关于对距离的衡量，没有关于对变量间标准差的衡量。比如决策树，他采用算法里面没有涉及到任何和距离等有关的，所以在做决策树模型时，通常是不需要将变量做标准化的。
- 另外，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。



</br>

## 分类算法

### 决策树

**决策树的构建**：通常都采用了**贪心策略**，**Hunt算法**是许多决策树算法的基础：

- 对于一个记录集，选择一个属性测试条件，将记录划分为较小的子集，然后对每个子集递归调用该算法，直到子集中所有记录都属于同一类（或纯度达到阈值）。

如何选择属性测试条件及最佳划分方法：使**增益Δ**最大，增益是父结点（划分前）的不纯程度和子女结点（划分后）的不纯程度的差：

$$
\Delta=I(parent)-\sum_{j=1}^{k}\frac{N(v_j)}{N}I(v_j)
$$

 $I(.)$ 是不纯性度量，包括（熵、Gini、分类误差）等方法，N是父结点的记录总数，k是属性值的个数。若选择熵作为不纯性度量，熵的差就叫**信息增益** **Δinfo**

**不纯性度量： $p(i|t)$ 表示给定结点 $t$ 中属于类 $i$ 的记录所占的比例**

- 熵： $Entropy(t)=-\sum_{i=0}^{c-1}p(i|t)log_2p(i|t)$
- Gini： $Gini(t)=1-\sum_{i=0}^{c-1}[p(i|t)]^2$
- 分类误差： $Classification\ error(t)=1-max_i[(p(i|t))]$

</br>

**各类属性划分的不纯性度量计算：**

- 二元属性、标称属性：按类计算即可

- 连续属性：首先将值排序，分别取其两相邻值的平均值点作为分隔点，按<=和>划分点来计算各划分点的不纯性，选择不纯性最小的划分方法。，


 </br>

**ID3树模型、C4.5树模型、CART树模型区别与联系:**

1. 划分标准
   - **ID3算法**：采用**信息增益**来选择能够最好地将样本分类的属性
   - **C4.5算法**：采用**增益率**（Gain ratio）的划分标准来评估划分， $Gain\ ratio=\frac{\Delta_{info}}{-\sum_{i=1}^{k}p(v_i)log_2p(v_i)}$ ，bigger is better
   - **CART算法**：采用**基尼系数最小化**来选择分类节点，选择**平方误差最小化**来选择回归节点，每次划分只能是二元划分，对 $k$ 个属性值，考虑产生 $2^k-1$ 种分裂方法。
2. ID3树模型、C4.5树模型只能用于分类任务处理离散特征，且生成的树可以是多叉树，由选择的划分特征的类别数决定，例如年龄包含青年、中年、老年三类，则分为三叉树。
3. CART树模型可用于分类任务处理离散特征，也可用于回归任务处理连续特征，但生成的树只能是二叉树，例如年龄包含青年、中年、老年三类，CART树将分为青年、非青年或中年、非中年或老年、非老年，只能分成二叉树。
4. ID3算法和C4.5算法根据对应的划分准则生成子结点后将使用的特征剔除，但CART算法是将使用的特征的对应值剔除，也就是说在CART算法中一个特征可以参与多次结点的生成，ID3算法和C4.5算法中每个特征最多只能参与一次结点的生成。例如年龄包含青年、中年、老年三类，CART使用青年和非青年来优先划分节点，划分后会将年龄特征中青年这个类别剔除，但是保留中年和老年类别且会参与后续节点的划分。

</br>

**决策树的特点**

- 是一种构建分类模型的非参数方法。
- 模型快速建立，建立后分类速度非常快，最坏情况下的时间复杂度是O(w)，w是树的最大深度。
- 决策树相对容易解释。在很多简单的数据集上，决策树的准确率可以与其他分类算法相媲美。
- 冗余属性不会对决策树的准确率造成不利的影响（冗余指和其他属性强相关）；但不相关属性过多可能会使决策树过于庞大。
- 叶结点的记录可能太少，对于叶结点代表的类，不能做出具有统计意义的判决，即**数据碎片**问题。一种可行解决方法是，当样本数小于特定阈值时停止分裂。
