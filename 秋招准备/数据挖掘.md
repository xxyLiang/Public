### 参数方法和非参数方法

- **参数方法：**在参数化方法中，我们通常根据先验知识假设模型服从函数f的分布，然后利用训练集估计出模型参数。
  - 参数化方法最大的缺点是，我们所做的**假设可能并不总是正确**的。例如，你可以假设函数的形式是线性的，但实际上它并不是。因此这些方法涉及较不灵活的算法，通常用于解决一些不复杂的问题。
  - 参数化方法**速度非常快**，而且它们**需要的数据也少得多**。此外，由于参数化方法虽然不太灵活但是因为基于我们做出的假设，所以它们更容易解释。
  - 机器学习中的参数化方法包括**线性判别分析、朴素贝叶斯和感知器**。
  
- **非参数方法：**指的是对于要估计的函数的形式不做任何潜在的假设的一组算法。由于没有做任何假设，这种方法可以估计未知函数f的任何形式。
  - 非参数方法往往**更精确**，因为它们寻求最佳拟合数据点。但是这是以需要进行**大量的观测为代价**的（这些观测是精确估计未知函数f所必需的）。并且这些方法在训练模型时往往效率较低。
  - 另外的一个问题是，非参数方法有时可能会引入**过拟合**，因为由于这些算法更灵活，它们有时可能会以无法很好地泛化到新的、看不见的数据点的方式学习错误和噪声。
  - 非参数方法**非常灵活**，因为没有对底层函数做出任何假设，所以可以带来**更好的模型性能**。
  - 机器学习中一些非参数方法的例子包括支持**决策树、向量机和kNN**。



### 模型的过分拟合

**分类模型的误差大致分为两种：**

- **训练误差**：也称再代入误差或表现误差，是在训练记录上误分类样本比例。
- **泛化误差**：是模型在未知记录上的期望误差。泛化误差的估计方法：
  - 使用训练误差（一种很差的估计）
  - 训练误差结合模型复杂度，相同训练误差但复杂度更小的模型更好
  - 训练误差的统计修正来估计
  - 使用验证集：保持方法、随机二次抽样、交叉验证、自助法（bootstrap）

过分拟合是指训练误差还在继续降低，但检验误差开始增大。

过分拟合的导致因素：噪声、缺乏代表性样本



### 数据变换

归一化 $x'=\frac{x-min(x)}{max(x)-min(x)}$

标准化 $x'=\frac{x-\bar{x}}{\sigma}$

中心化 $x'=x-\bar{x}$ ，变换后特征x'的均值为0

归一化和标准化的区别：

- 归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。
- 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，会改变数据分布（均值和标准差改变，但分布类型不变），和整体样本分布相关，每个样本点都能对标准化产生影响。
- 它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。
