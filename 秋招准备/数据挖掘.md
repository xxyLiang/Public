### 参数方法和非参数方法

- **参数方法**：在参数化方法中，我们通常根据先验知识假设模型服从函数f的分布，然后利用训练集估计出模型参数。
  - 参数化方法最大的缺点是，我们所做的**假设可能并不总是正确**的。例如，你可以假设函数的形式是线性的，但实际上它并不是。因此这些方法涉及较不灵活的算法，通常用于解决一些不复杂的问题。
  - 参数化方法**速度非常快**，而且它们**需要的数据也少得多**。此外，由于参数化方法虽然不太灵活但是因为基于我们做出的假设，所以它们更容易解释。
  - 机器学习中的参数化方法包括**线性判别分析、朴素贝叶斯和感知器**。
  
- **非参数方法**：指的是对于要估计的函数的形式不做任何潜在的假设的一组算法。由于没有做任何假设，这种方法可以估计未知函数f的任何形式。
  - 非参数方法往往**更精确**，因为它们寻求最佳拟合数据点。但是这是以需要进行**大量的观测为代价**的（这些观测是精确估计未知函数f所必需的）。并且这些方法在训练模型时往往效率较低。
  - 另外的一个问题是，非参数方法有时可能会引入**过拟合**，因为由于这些算法更灵活，它们有时可能会以无法很好地泛化到新的、看不见的数据点的方式学习错误和噪声。
  - 非参数方法**非常灵活**，因为没有对底层函数做出任何假设，所以可以带来**更好的模型性能**。
  - 机器学习中一些非参数方法的例子包括支持**决策树、向量机和kNN**。



### 模型的过分拟合

**分类模型的误差大致分为两种：**

- **训练误差**：也称再代入误差或表现误差，是在训练记录上误分类样本比例。
- **泛化误差**：是模型在未知记录上的期望误差。泛化误差的估计方法：
  - 使用训练误差（一种很差的估计）
  - 训练误差结合模型复杂度，相同训练误差但复杂度更小的模型更好
  - 训练误差的统计修正来估计
  - 使用验证集：保持方法、随机二次抽样、交叉验证、自助法（bootstrap）

过分拟合是指训练误差还在继续降低，但检验误差开始增大。

过分拟合的导致因素：噪声、缺乏代表性样本

</br>

#### 正则化

正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。

在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。

在目标函数后面添加一个系数的“**惩罚项**”是正则化的常用方式，为了防止系数过大从而让模型变得复杂。

1. L1正则化时，惩罚项为： $\Omega(w)=\|w\|_1=\sum_i|w_i|$ 。L1正则化倾向于聚集网络的权重在相对少量的高重要度连接上，而其他权重就会被驱使向 0 接近。
2. L2正则化时，惩罚项为： $\Omega(w)=\|w\|_2^2=\sum_i|w_i^2|$ 。L2正则化不使每个元素都为0，而是接近于零。越小的参数模型越简单，越简单的模型越不容易产生过拟合现象。

</br>

### 数据变换

归一化： $x'=\frac{x-min(x)}{max(x)-min(x)}$

标准化： $x'=\frac{x-\bar{x}}{\sigma}$

中心化： $x'=x-\bar{x}$ ，变换后特征x'的均值为0

log变换 ： $x'=\frac{log(x)}{log(max(x))}$

sigmoid变换： $x'=\frac{1}{1+e^{-x}}$

softmax变换： $x'=\frac{e^x}{\sum_{i=1}^n e^{x_i}}$



#### 归一化和标准化的区别

- 归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。
- 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，会改变数据分布（均值和标准差改变，但分布类型不变），和整体样本分布相关，每个样本点都能对标准化产生影响。
- 它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。



#### 归一化和标准化的原因、用途

1. 统计建模中，如回归模型，自变量 $X$ 的量纲不一致导致了回归系数无法直接解读或者错误解读；需要将 $X$ 都处理到统一量纲下，这样才可比；
2. 机器学习任务和统计学任务中有很多地方要用到“距离”的计算，比如PCA，比如KNN，比如kmeans等等，假使算欧式距离，不同维度量纲不同可能会导致距离的计算依赖于量纲较大的那些特征而得到不合理的结果；
3. 参数估计时使用梯度下降，在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，即提升模型的收敛速度。



#### 归一化和标准化的选择

- 如果你对处理后的数据范围有严格要求，那肯定是归一化。
- 如果数据不为稳定，存在极端的最大最小值，不要用归一化。
- 标准化是ML中更通用的手段，如果你无从下手，可以直接使用标准化；
- 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化表现更好；在不涉及距离度量、协方差计算的时候，可以使用归一化方法。



#### 是否所有情况都需要归一化和标准化

- 当原始数据不同维度特征的尺度（量纲）不一致时，需要标准化步骤对数据进行标准化或归一化处理，反之则不需要进行数据标准化。
- 也不是所有的模型都需要做归一的，比如模型算法里面有没关于对距离的衡量，没有关于对变量间标准差的衡量。比如决策树，他采用算法里面没有涉及到任何和距离等有关的，所以在做决策树模型时，通常是不需要将变量做标准化的。
- 另外，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。

</br>

### 损失函数

平方误差： $L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2$ ，常应用于回归问题。

交叉熵： $L(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))$ ，本质上是一种对数似然函数，可用于二分类和多酚类任务中。

Hinge损失函数： $L(\hat{y},y)=max(0,1-y\hat{y})$ ，用于SVM

</br>

**极大似然估计**

极大似然估计，通俗理解来说，**就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！**

对于这个函数： $p(x|θ)$ 输入有两个： $x$ 表示某一个具体的数据； $θ$ 表示模型的参数

- 如果 $θ$ 是已知确定的， $x$ 是变量，这个函数叫做**概率函数(probability function)**，它描述对于不同的样本点 $x$ ，其出现概率是多少。
- 如果 $x$ 是已知确定的， $θ$ 是变量，这个函数叫做**似然函数(likelihood function)**, 它描述对于不同的模型参数，出现 $x$ 这个样本点的概率是多少。

在逻辑回归模型中，我们**最大化似然函数**和**最小化损失函数**实际上是等价的。

</br>

## 分类算法

### 决策树

**决策树的构建**：通常都采用了**贪心策略**，**Hunt算法**是许多决策树算法的基础：

- 对于一个记录集，选择一个属性测试条件，将记录划分为较小的子集，然后对每个子集递归调用该算法，直到子集中所有记录都属于同一类（或纯度达到阈值）。

如何选择属性测试条件及最佳划分方法：使**增益Δ**最大，增益是父结点（划分前）的不纯程度和子女结点（划分后）的不纯程度的差：

$$
\Delta=I(parent)-\sum_{j=1}^{k}\frac{N(v_j)}{N}I(v_j)
$$

 $I(.)$ 是不纯性度量，包括**熵、Gini、分类误差**等方法，N是父结点的记录总数，k是属性值的个数。若选择熵作为不纯性度量，熵的差就叫**信息增益 $\Delta info$ **

**不纯性度量**： $p(i|t)$ 表示给定结点 $t$ 中属于类 $i$ 的记录所占的比例
$$
Entropy(t)=-\sum_{i=0}^{c-1}p(i|t)log_2p(i|t)
$$

$$
Gini(t)=1-\sum_{i=0}^{c-1}[p(i|t)]^2
$$

$$
Classification\ error(t)=1-max_i[(p(i|t))]
$$

</br>

**各类属性划分的不纯性度量计算：**

- 二元属性、标称属性：按类计算即可

- 连续属性：首先将值排序，分别取其两相邻值的平均值点作为分隔点，按<=和>划分点来计算各划分点的不纯性，选择不纯性最小的划分方法。，


 </br>

**ID3树模型、C4.5树模型、CART树模型区别与联系:**

1. 划分标准
   - **ID3算法**：采用**信息增益**来选择能够最好地将样本分类的属性
   - **C4.5算法**：采用**增益率**（Gain ratio）的划分标准来评估划分， $Gain\ ratio=\frac{\Delta_{info}}{-\sum_{i=1}^{k}p(v_i)log_2p(v_i)}$ ，bigger is better
   - **CART算法**：采用**基尼系数最小化**来选择分类节点，选择**平方误差最小化**来选择回归节点，每次划分只能是二元划分，对 $k$ 个属性值，考虑产生 $2^k-1$ 种分裂方法。
2. ID3树模型、C4.5树模型只能用于分类任务处理离散特征，且生成的树可以是多叉树，由选择的划分特征的类别数决定，例如年龄包含青年、中年、老年三类，则分为三叉树。
3. CART树模型可用于分类任务处理离散特征，也可用于回归任务处理连续特征，但生成的树只能是二叉树，例如年龄包含青年、中年、老年三类，CART树将分为青年、非青年或中年、非中年或老年、非老年，只能分成二叉树。
4. ID3算法和C4.5算法根据对应的划分准则生成子结点后将使用的特征剔除，但CART算法是将使用的特征的对应值剔除，也就是说在CART算法中一个特征可以参与多次结点的生成，ID3算法和C4.5算法中每个特征最多只能参与一次结点的生成。例如年龄包含青年、中年、老年三类，CART使用青年和非青年来优先划分节点，划分后会将年龄特征中青年这个类别剔除，但是保留中年和老年类别且会参与后续节点的划分。

</br>

**决策树的特点**

- 是一种构建分类模型的**非参数方法**。
- 模型快速建立，建立后分类**速度非常快**，最坏情况下的时间复杂度是O(w)，w是树的最大深度。
- 决策树相对**容易解释**。在很多简单的数据集上，决策树的准确率可以与其他分类算法相媲美。
- 冗余属性不会对决策树的准确率造成不利的影响（冗余指和其他属性强相关）；但不相关属性过多可能会使决策树过于庞大。
- 叶结点的记录可能太少，对于叶结点代表的类，不能做出具有统计意义的判决，即**数据碎片**问题。一种可行解决方法是，当样本数小于特定阈值时停止分裂。

</br>

### 基于规则的分类

基于规则的分类器是使用一组“if…then…”规则来对记录进行分类的技术。

特点

- 直线决策边界
- 规则集的**表达能力**几乎等价于决策树，因为决策树可以用互斥和穷举的规则集表示。
- 基于规则的分类器通常被用来产生**更易于解释**的描述性模型，而模型的性能却可与决策树分类器相媲美。
- 被很多基于规则的分类器所采用的基于类的规则定序方法非常适于处理**类分布不平衡**的数据集。

</br>

### 最近邻分类

最近邻分类（k Nearing Neighbor, kNN）是一种**消极学习方法**，直到需要分类测试样例时再进行训练数据的建模。

**测试样例的类是其k-近邻实例的多数表决类。**（一种改进是距离加权表决）

- K值较小，则模型复杂度较高，容易发生过拟合，学习的估计误差会增大，预测结果对近邻的实例点非常敏感。
- K值较大可以减少学习的估计误差，但是学习的近似误差会增大，与输入实例较远的训练实例也会对预测起作用，使预测发生错误，k值增大模型的复杂度会下降。
- 在应用中，k值一般取一个比较小的值，通常采用交叉验证法来来选取最优的K值。

特点

- 是一类基于实例学习的技术。需要采用适当的邻近性度量和**数据预处理**（如标准化、归一化等），否则结果会被绝对值大的属性左右。
- 不需要建立模型，但分类**测试样例时开销很大**。
- 局部分类决策，（k很小时）**对噪声非常敏感**。
- 可以生成**任意形状**的决策边界

</br>

### 贝叶斯分类器

#### 朴素贝叶斯分类器

1. 朴素贝叶斯分类器在估计类条件概率时假设**属性之间条件独立**：

$$
P(\mathbf{X}|Y=y)=\prod_{i=1}^{d}P(X_i|Y=y)
$$

- ​	连续属性的条件概率：
  - 可以把每一个连续的属性离散化，然后用相应的离散区间替换连续属性值。
  - 可以假设连续变量服从某种概率分布，使用训练数据估计分布的参数，通常使用高斯分布：

$$
P(X_i=x_i|Y=y_j)=\frac{1}{\sqrt{2\pi}\sigma_{ij}}e^{-\frac{(x_i-\mu_{ij})^2}{2\sigma_{ij}^2}}
$$

2. **如何工作：最大化以下等式的分子**

$$
P(Y|\mathbf{X})=\frac{P(Y)\prod_{i=1}^{d}P(X_i|Y)}{P(\mathbf{X})}
$$

特点

- 面对**孤立的噪声点**，朴素贝叶斯分类器是健壮的。
- 面对**无关属性**，该分类器是健壮的。（无关属性 $X_i$ 的 $P(X_i|Y)$ 几乎变成了均匀分布）
- **相关属性**可能会降低朴素贝叶斯分类器的性能，因为对这些属性，条件独立的假设已不成立。

</br>

#### 贝叶斯信念网络

相对于朴素贝叶斯假设所有属性之间独立，**贝叶斯信念网络允许指定哪些属性条件独立，哪些不独立**。有两个主要成分：

1. 一个有向无环图，表示变量之间的依赖关系
2. 一个概率表，把各结点和它的直接父结点关联起来

特征

- **构造网络可能费时费力**，但一旦网络结构确定了，添加新变量就十分容易
- 贝叶斯网络很**适合处理不完整的数据**。对有属性遗漏的实例可以通过对该属性的所有可能取值的概率求和或求积分来加以处理。
- 由于数据和先验知识以概率的方式结合起来了，该方法对模型的**过拟合问题非常健壮**。

</br>

### 支持向量机

支持向量机（Support Vector Machine, SVM）是一类按监督学习方式对数据进行二元分类的广义线性分类器，其**决策边界**是对学习样本求解的**最大边距超平面**，距离超平面最近的且满足一定条件的几个训练样本点被称为**支持向量**。

- **线性可分SVM**：当数据线性可分的时候，通过**硬间隔最大化**可以学习得到一个线性分类器，即硬间隔SVM。
- **线性SVM**：当训练数据不能线性可分但是可以近似线性可分时，通过**软间隔最大化**也可以学习到一个线性分类器，即软间隔SVM。引入了正值的松弛变量 $\xi_i$ （可看作是错误样本与决策边界的相对距离），训练的惩罚项为

$$
f(\mathbf{w})=\frac{\|\mathbf{w}\|^2}{2}+C\lgroup\sum_{i=1}^{N}\xi_i\rgroup^k
$$

​	其中 $C$ 和 $k$ 是用户指定的参数

- **非线性SVM**：当训练数据线性不可分时，通过使用**核技巧**（kernel trick）和**软间隔最大化**，可以学习到一个非线性SVM。



**核函数**：支持向量机通过某非线性变换 $\varphi(x)$ ，将输入空间映射到高维特征空间。特征空间的维数可能非常高，可能产生维灾难。为解决这个问题，引入核函数：如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数 $K(x, x^′) $，它恰好等于在高维空间中这个内积，即 $K(x,x^′)=\langle\varphi(x) ⋅\varphi(x^′) \rangle$。为了实现上述条件，核函数必须满足Mercer定理的数学原理。**核函数它本质上隐含了从低维到高维的映射，从而避免直接计算高维的内积。**

**常用的核函数**

- 线性核函数 Linear： $K(\mathbf{X_1},\mathbf{X_2})=\langle\mathbf{X_1},\mathbf{X_2}\rangle$ 
- 高斯核函数 / 径向基函数 rbf： $K(\mathbf{X_1},\mathbf{X_2})=e^{-\gamma\|\mathbf{X_1}-\mathbf{X_2}\|^2}$ 
- 多项式核函数 poly： $K(\mathbf{X_1},\mathbf{X_2})=(\gamma\langle\mathbf{X_1},\mathbf{X_2}\rangle+c)^n$ 

**核函数的选择**

- 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
- 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
- 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

</br>

**SVM的多元分类方法**

- 分解成 K 个二类问题，第 $i$ 个分类器决定 $y$ 是否属于 $y_i$ 。这种方法称为一对其他（1-r）方法。
- 构建 K(K-1)/2 个二类分类器，每一个分类器用来区分一对类 $(y_i,y_j)$ 。当为类 $(y_i,y_j)$ 构建二类分类器时，不属于这两类的样本被忽略掉。这种方法称为一对一（1-1）方法。

组合预测的典型做法是使用多数表决。

</br>

**特点**

- SVM学习问题可以表示为凸优化问题，因此**可以发现全局最小值**，而其他如神经网络方法只能获得局部最优解
- SVM通过最大化决策边界的边缘来控制模型的能力，用户需要提供核函数类型、为了引入松弛变量所需的代价函数C等。

</br>

**svm vs. 感知机**

感知机与SVM一样都是使用超平面对空间线性可分的向量进行分类，不同的是：**感知机的目标是尽可能将所有样本分类正确，这种策略指导下得出的超平面可能有无数个，然而SVM不仅需要将样本分类正确，还需要最大化最小分类间隔**



### 逻辑回归

Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。

Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

Logistic分布的分布函数：

$$
F(x)=P(X\le x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}
$$

<img src="./图片/lr.jpeg" alt="lr" style="zoom:67%;" />

Logistic 回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率，其优点有：

1. 直接对**分类的概率**建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；
2. 不仅可预测出类别，还能得到该**预测的概率**，这对一些利用概率辅助决策的任务很有用；
3. 对数几率函数是**任意阶可导的凸函数**，有许多数值优化算法都可以求出最优解。
